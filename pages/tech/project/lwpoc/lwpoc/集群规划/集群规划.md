

## 1 集群组件

| 组件        | 版本                                                         | 备注                                                         |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Iceberg     | [0.12.0](https://iceberg.apache.org/releases/)               |                                                              |
| Hadoop/HDFS | 3.1.1                                                        | HDP 3.1.5 中集成                                             |
| Flink       | [1.12.5](https://flink.apache.org/downloads.html)            | [Flink - Apache Iceberg](https://iceberg.apache.org/flink/)，根据 iceberg 兼容性适配版本 |
| Spark       | [3.1.2](https://www.apache.org/dyn/closer.lua/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz) | TiDB 目前支持 2.4 版本的 Spark，HDP 3.1.5是 2.3 版本；考虑单独部署一个3.1.2 版本的 Spark |
| Pulsar      | 2.8.0                                                        |                                                              |
| Trino       | 360                                                          |                                                              |
| TiDB        | 5.2.0                                                        |                                                              |
| HDP         | 3.1.5                                                        |                                                              |
| Ambari      | 2.7.5                                                        |                                                              |



## 2 主机规划

### host 规划

整体划分为bdp和ti两部分：

- **生产环境**

  - **公共管理节点**：public.mgmt01

  - **大数据集群**
    - 管理节点：bdp.mgmt01、bdp.mgmt02、bdp.mgmt03
    - 存储节点：bdp.sto01、bdp.sto02、bdp.sto03、bdp.sto04、bdp.sto05
    - 计算节点：bdp.cpt01、bdp.cpt02、bdp.cpt03、bdp.cpt04、bdp.cpt05、bdp.cpt06、bdp.cpt07、bdp.cpt08、bdp.cpt09、bdp.cpt.n10
  - **TI-DB集群**
    - tidb：ti.db01、ti.db02、ti.db03
    - tikv：ti.kv01、ti.kv02、ti.kv03
    - tiflash：ti.flash01、ti.flash02、ti.flash03
    - ticdc:  ti.cdc01、ti.cdc02

- **开发环境**
- **公共管理节点**：dev.public.mgmt01
  
- **大数据集群**
    - 管理节点：dev.bdp.mgmt01、dev.bdp.mgmt02、dev.bdp.mgmt03
    - 计算节点：dev.bdp.cpt01、dev.bdp.cpt02、dev.bdp.cpt03
  - **TI-DB集群**
    - tidb：dev.ti.db01、dev.ti.db02、dev.ti.db03
    - tiflash: dev.ti.flash01、dev.ti.flash02


### 部署目录

- 安装目录规划：/deploy/{module} ,其中module指代具体组件名称，如trino。



## 3 TiDB 集群规划

> 按现有数据仓库存储数据量 2.5T，3 副本，60% （总 2*6T）负载规划。



### 硬件规划

| 组件        | CPU    | 内存    | 硬盘                                   | 网络                 | 实例数量 | 备注                                             |
| ----------- | ------ | ------- | -------------------------------------- | -------------------- | -------- | ------------------------------------------------ |
| TiDB        | 16 核+ | 32 GB+  | 1 * 200G SAS                           | 万兆网卡（2 块最佳） | 2        | 需要开启 TiDB Binlog 磁盘需要大一点              |
| PD          | 4核+   | 8 GB+   | 1 * 200G SSD                           | 万兆网卡（2 块最佳） | 3        |                                                  |
| TiKV        | 16 核+ | 32 GB+  | 1 * <2T PCI-E SSD                      | 万兆网卡（2 块最佳） | 6        | 12 T，3 副本，规划存储 4T                        |
| TiFlash     | 48 核+ | 128 GB+ | 1 * 200G PCI-E SSD + [1,2]* 2T SSD/SAS | 万兆网卡（2 块最佳） | [2,4]    | 8.5T，2 副本，规划存储 4T，主存储盘可以使用 SAS  |
| ~~TiSpark~~ | 8 核+  | 16 GB+  | ？                                     | ？                   | 3        | 规划在Spark 集群                                 |
| TiCDC       | 16 核+ | 64 GB+  | 1 * 200G PCIE-SSD                      | 万兆网卡（2 块最佳） | 2        |                                                  |
| 监控        | 8 核+  | 16 GB+  | 1 * 500G SAS                           | 千兆网卡             | 1        |                                                  |
| HAProxy/LVS | 8 核+  | 16 GB+  | 1 * 100G SAS                           | 万兆网卡             | 2        | 负载均衡组件，HA 方式部署；可以与TiDB 部署在一起 |

**注意：**

- 生产环境中的 TiDB 和 PD 可以部署和运行在同服务器上，如对性能和可靠性有更高的要求，应尽可能分开部署。
- 生产环境强烈推荐使用更高的配置。
- TiKV 硬盘大小配置建议 PCI-E SSD 不超过 2 TB，普通 SSD 不超过 1.5 TB。
- TiFlash 支持[多盘部署](https://docs.pingcap/zh/tidb/dev/tiflash-configuration#多盘部署)。
- TiFlash 数据目录的第一块磁盘推荐用高性能 SSD 来缓冲 TiKV 同步数据的实时写入，该盘性能应不低于 TiKV 所使用的磁盘，比如 PCI-E SSD。并且该磁盘容量建议不小于总容量的 10%，否则它可能成为这个节点的能承载的数据量的瓶颈。而其他磁盘可以根据需求部署多块普通 SSD，当然更好的 PCI-E SSD 硬盘会带来更好的性能。
- TiFlash 推荐与 TiKV 部署在不同节点，如果条件所限必须将 TiFlash 与 TiKV 部署在相同节点，则需要适当增加 CPU 核数和内存，且尽量将 TiFlash 与 TiKV 部署在不同的磁盘，以免互相干扰。
- TiFlash 硬盘总容量大致为：`整个 TiKV 集群的需同步数据容量 / TiKV 副本数 * TiFlash 副本数`。例如整体 TiKV 的规划容量为 1 TB、TiKV 副本数为 3、TiFlash 副本数为 2，则 TiFlash 的推荐总容量为 `1024 GB / 3 * 2`。用户可以选择同步部分表数据而非全部，具体容量可以根据需要同步的表的数据量具体分析。
- TiCDC 硬盘配置建议 200 GB+ PCIE-SSD。



### 混合部署规划（生产）

| 组件         | CPU    | 内存    | 硬盘                                            | 网络                 | 实例数量 | 备注                                             |
| ------------ | ------ | ------- | ----------------------------------------------- | -------------------- | -------- | ------------------------------------------------ |
| TiDB/PD/TiKV | 32 核+ | 64 GB+  | 1 * 200G SAS + 1 * 200G SSD + 1 * <2T PCI-E SSD | 万兆网卡（2 块最佳） | 3        | 需要开启 TiDB Binlog 磁盘需要大一点              |
| TiKV         | 16 核+ | 32 GB+  | 1 * <2T PCI-E SSD                               | 万兆网卡（2 块最佳） | 3        | 12 T，3 副本，规划存储 4T                        |
| TiFlash      | 48 核+ | 128 GB+ | 1 * 200G PCI-E SSD + 1 * 2.5T SSD/SAS           | 万兆网卡（2 块最佳） | 3        | 7.5T，2 副本，规划存储 3.75T                     |
| TiCDC        | 16 核+ | 64 GB+  | 1 * 200G PCIE-SSD                               | 万兆网卡（2 块最佳） | 2        |                                                  |
| 监控         | 8 核+  | 16 GB+  | 1 * 500G SAS                                    | 千兆网卡             | 1        |                                                  |
| HAProxy/LVS  | 8 核+  | 16 GB+  | 1 * 100G SAS                                    | 万兆网卡             | 2        | 负载均衡组件，HA 方式部署；可以与TiDB 部署在一起 |

### TiDB 集群部署拓扑

#### 部署拓扑逻辑图

***`图 2-1，TiDB 集群拓扑-逻辑架构`***

![image-20210811091038832](./assets/arch-TiDB集群拓扑（逻辑）.drawio.svg)

#### 部署拓扑物理图（混合部署）

> ​		在对性能要求不高且需要控制成本的场景下，将 TiDB、TiKV、PD 混合部署在三台机器上是一个可行的方案。



***`图 2-2，TiDB 集群拓扑-物理混合部署架构`***

![image-20210824171632297](./assets/arch-TiDB集群拓扑（物理混合）.drawio.svg)



## 4 大数据平台集群规划

### 硬件规划

| 组件                                                         | CPU       | 内存                                                         | 硬盘                                        | 网络     | 实例数量 | 备注                                                         |
| ------------------------------------------------------------ | --------- | ------------------------------------------------------------ | ------------------------------------------- | -------- | -------- | ------------------------------------------------------------ |
| Ambari                                                       |           |                                                              |                                             |          |          |                                                              |
| 管理节点（HDFS NameNode、JournalNode、Yarn  ResourceManager，Zookeeper、Trino Coordinator节点等） | 32核+     | 256GB+                                                       | 3*2TB,  SAS，7200RPM，JBOD 设置，无 RAID    | 万兆     | 3        | NameNode和Coordinator占用主要的内存和CPU，磁盘主要是存储HDFS的fsimage和editlog，namenode的本地目录可以配置多个，每个目录相同且配置到不同的磁盘，文件更新时会同时更新配置的目录，增加可靠性 |
| HDFS 管理节点                                                | 16核+     | 128GB+                                                       | 2*2TB,  SAS，7200RPM，RAID1                 | 万兆     | 2        | 合并到管理节点中                                             |
| HDFS 数据节点                                                | 12核      | 64GB+                                                        | 14*1.8TB,  SAS，7200RPM，JBOD 设置，无 RAID | 万兆     | [5-8]    | 当前-5，半年-6，一年-7，两年-8                               |
| 处理节点（Yarn-NodeManager、Spark、Flink、Mapreduce2）       | 64核+     | 256 +256 GB+                                                 | 1*2TB, SSD                                  | 万兆+    | 5        | Spark会将中间过程数据落地到磁盘，使用SSD可以大大加速的效果。 |
| Hive                                                         |           |                                                              |                                             |          |          | 未来应用场景少，安装但先不做资源规划                         |
| Yarn-Spark                                                   | 32核+     | 256GB+                                                       | 1*2TB, SSD                                  | 万兆+    | 5        | spark的使用场景是tidb和数据湖的数据交换、tidb集群上数据的分析 |
| Yarn-Flink                                                   | 2* 16 核+ | 2*  16 核+                                                   | 1  * 200 SAS                                | 万兆     | 4        | 目标值：     1. 处理能力 130M/s     2. 同时支持运行JOB数100， 平均时间窗口为20 S     3. 平均计算算计节点为5个（1个source/1个sink/map和checkpoint 为3个） |
| Trino Coordinator节点                                        | 16核+     | 60GB+                                                        | 200GB，SAS，7200RPM，RAID1                  | 万兆+    | 2        | 磁盘主要用于存储日志和其他数据,Coordinator存在单节点故障，需要调研解决方案，为了扩展高可用规划为2 |
| Trino Worker节点                                             | 48核+     | 256GB+                                                       | 2*1TB,  SAS，7200RPM，JBOD 设置，无 RAID    | 万兆+    | 5        | 按照20并行任务，单个任务最高消耗分布式内存为50GB规划；Trino  on Yarn:  Trino成员建议独立运行Trino，不建议与其上的其他组件共享节点，目前实现方案是在slider上实现的，与slider的集成很复杂且难以维护。为此暂时先不考虑Trino  on Yarn架构; |
| Broker/Bookie/Proxy                                          | 64  核+   | 128  GB+     Broker：32 GB         heap: 10GB        direct memory: 22GB     Bookie：32 GB         heap: 10GB        direct memory: 22GB     PageCache: 64GB | 8  * 1T SSD/HDD                             | 万兆网卡 | 3        | 24T，2副本，规划存储  12T                                    |

### 集群部署拓扑

#### 部署拓扑逻辑图

![image-20210824172348455](./assets/arch-大数据平台拓扑（逻辑）.drawio.svg)

#### 部署拓扑物理图

![image-20210824172503001](./assets/arch-大数据平台拓扑（物理）.drawio.svg)







# REF

- [Installation Guide for Ambari 2.7.5 - Apache Ambari - Apache Software Foundation](https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.7.5)
- [CDH6.3.2离线安装（附百度网盘CDH安装包） - 知乎 (zhihu)](https://zhuanlan.zhihu/p/366308900)
- [HDP Component Versions (cloudera)](https://docs.cloudera/HDPDocuments/HDP3/HDP-3.1.5/release-notes/content/comp_versions.html)
- [ambari2.7搭建hadoop3.0_一蓑烟雨-CSDN博客_ambari hadoop](https://blog.csdn.net/lanlianhua_luffy/article/details/103914183)
- [最新版Ambari2.75安装及HDP3.1.5集群搭建_NDF923的专栏-CSDN博客](https://blog.csdn.net/NDF923/article/details/116445785)
- [产品概述 智能大数据平台 USDP_文档中心_UCloud中立云计算服务商](https://docs.ucloud.cn/USDP/README)
- [通过拓扑 label 进行副本调度 | PingCAP Docs](https://docs.pingcap/zh/tidb/v4.0/schedule-replicas-by-topology-labels)
-  [【SOP 系列 04】现有集群 TiKV 如何从单实例部署调整为多实例部署 - 新手区 / TiDB 运维手册 - AskTUG](https://asktug/t/topic/33167)
-  [TiDB 整体架构 | PingCAP Docs](https://docs.pingcap/zh/tidb/stable/tidb-architecture)
-  [HAProxy 在 TiDB 中的最佳实践 | PingCAP Docs](https://docs.pingcap/zh/tidb/stable/haproxy-best-practices#haproxy-在-tidb-中的最佳实践)
-  [基于 LVS + Keepalived 搭建负载均衡 - Dudashuang | 汪文超](https://dudashuang/load-blance/)
-  [TiSpark 用户指南 | PingCAP Docs](https://docs.pingcap/zh/tidb/stable/tispark-overview)
-  [TiSpark 服务安装、部署及测试 - 技术文章 / 运维实战 - AskTUG](https://asktug/t/topic/149)
-  [三节点混合部署最佳实践 | PingCAP Docs](https://docs.pingcap/zh/tidb/stable/three-nodes-hybrid-deployment)
-  [从0开始建设大数据平台](https://blog.51cto/wang/2945265)
-  [通过 TiUP 部署 TiDB 集群的拓扑文件配置 | PingCAP Docs](https://docs.pingcap/zh/tidb/stable/tiup-cluster-topology-reference)



**本页编辑**      **[@gongshiwen](http://192.168.1.23/gongshiwen)** <img src="http://192.168.1.23/uploads/-/system/user/avatar/10/avatar.png?width=100" style="zoom:10%;" /> 