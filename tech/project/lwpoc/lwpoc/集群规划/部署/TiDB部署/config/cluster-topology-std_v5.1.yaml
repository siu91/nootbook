# 标准集群拓扑配置文件
# 创建时间：2021/8/12 15:51
# 创建者：Siu
# 最后更新时间：2021/8/13 16:28

## 修改记录
### 2021/8/12 v1.0 <集群拓扑文件初版和 tidb-server 参数调优> Siu
### 2021/8/13 v1.1 <tikv、tiflash 参数调优> Siu
### 2021/8/16 v1.2 <配置label副本调度> Siu
### 2021/8/16 v1.3 <max-store-down-time 最佳实践> Siu

# 拓扑文件参考： https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference
# 配置项参考： https://docs.pingcap.com/zh/tidb/stable/tidb-configuration-file
# 通过拓扑 label 进行副本调度：https://docs.pingcap.com/zh/tidb/stable/schedule-replicas-by-topology-labels
# 下推计算结果缓存： https://docs.pingcap.com/zh/tidb/stable/coprocessor-cache
# Placement Rules 使用文档：https://docs.pingcap.com/zh/tidb/stable/configure-placement-rules
# Split Region 使用文档（新建的表上发生大批量写入，则会造成热点）：https://docs.pingcap.com/zh/tidb/stable/sql-statement-split-region
# PD 调度策略最佳实践 https://docs.pingcap.com/zh/tidb/stable/pd-scheduling-best-practices#region-merge
# Load Base Split（基于统计信息自动拆分 Region） 使用文档： https://docs.pingcap.com/zh/tidb/stable/configure-load-base-split

# 注意，以下参数部署完成之后，禁止修改：
#
#  host
#  web_port
#  cluster_port
#  deploy_dir
#  data_dir
#  log_dir
#  arch
#  os

# label 副本调度， 分为三层：机房（zone) -> 机架（rack）-> 主机（host）
# 在无多个数据中心的情况下，配置两个层级 rack/host

# 集群全局配置，其中一些是集群的默认值，可以在实例里面单独配置
# https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference#global
global:
  user: "tidb"
  ssh_port: 22
  deploy_dir: "/tidb-deploy"
  data_dir: "/tidb-data"

# 监控服务配置
# https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference#monitored
# 略

# 组件全局配置，可单独针对每个组件配置，若在实例中存在同名配置项，那么以实例中配置的为准
# https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference#server_configs
server_configs:
  # tidb-server 配置参数：https://docs.pingcap.com/zh/tidb/stable/tidb-configuration-file
  tidb:
    # 慢日志阈值，毫秒
    log.slow-threshold: 300
    # 日志保存周期，天
    log.file.max-days: 30
    # 是否开启 binlog
    binlog.enable: false
    # 单行数据的大小限制，默认值：6291456（6MB）,该配置项的最大值不超过 125829120（120MB）
    # 注意，TiKV 有类似的限制。若单个写入请求的数据量大小超出 raft-entry-max-size，默认为 8MB，TiKV 会拒绝处理该请求。当表的一行记录较大时，需要同时修改这两个配置。
    performance.txn-entry-size-limit: 125829120
    # performance.txn-total-size-limit 单个事务大小限制,默认值：104857600（100MB），该配置项的最大值不超过 10737418240（10GB）；需要同时修改 raft-entry-max-size
    performance.txn-total-size-limit: 1073741824 # 1GB
    # performance.cross-join 默认值：true，默认可以执行在做 join 时两边表没有任何条件（where 字段）的语句；如果设置为 false，则有这样的 join 语句出现时，server 会拒绝执行
    # performance.cross-join: false
    # table-column-count-limit 用于设置单个表中列的数量限制，默认值：1017，合法值范围 [1017, 4096]
    table-column-count-limit: 4096
    # split-table 为每个 table 建立单独的 Region,默认值：true;如果需要创建大量的表，建议将此参数设置为 false；ODS 层的表也放在 TiDB 需要设置成 false
    # split-table: false
    # enable-telemetry 是否开启 TiDB 遥测功能,默认值：true;当 TiDB 遥测功能开启时，TiDB 集群将会以 6 小时为周期收集使用情况信息并分享给 PingCAP
    enable-telemetry: false
    # oom-action 可选配置 ["log", "cancel"]，默认 "cancel"；当 TiDB 中单条 SQL 的内存使用超出 mem-quota-query 限制且不能再利用临时磁盘时的行为："log" 时，仅输出日志。设置为 "cancel" 时，取消执行该 SQL 操作，并输出日志
    # oom-action = "log"
    # enable-tcp4-only 控制是否只监听 TCP4，默认值：false；当使用 LVS 为 TiDB 做负载均衡时，可开启此配置项。这是因为 LVS 的 TOA 模块可以通过 TCP4 协议从 TCP 头部信息中解析出客户端的真实 IP。
    # enable-tcp4-only: true
    # graceful-wait-before-shutdown 指定关闭服务器时 TiDB 等待的秒数，使得客户端有时间断开连接,默认值：0;在 TiDB 等待服务器关闭期间，HTTP 状态会显示失败，使得负载均衡器可以重新路由流量。
    # graceful-wait-before-shutdown: 5
  tikv:
    # grpc-concurrency gRPC 工作线程的数量；默认值：5，最小值：1
    # 如果部署的机器 CPU 核数特别少（小于等于 8），可以考虑将该配置（server.grpc-concurrency）设置为 2
    # 如果机器配置很高，并且 TiKV 承担了非常大量的读写请求，观察到 Grafana 上的监控 Thread CPU 的 gRPC poll CPU 的数值超过了 server.grpc-concurrency 大小的 80%，那么可以考虑适当调大 server.grpc-concurrency 以控制该线程池使用率在 80% 以下（即 Grafana 上的指标低于 80% * server.grpc-concurrency 的值）
    # server.grpc-concurrency: 2
    # raftstore.store-pool-size 处理 raft 的线程池线程数，默认值：2；最好控制其整体 CPU 使用在 60% 以下（按照线程数默认值 2，则 Grafana 监控上的 TiKV-Details.Thread CPU.Raft store CPU 上的数值控制在 120% 以内较为合理）。不要为了提升写性能盲目增大 Raftstore 线程池大小，这样可能会适得其反，增加了磁盘负担让性能变差。
    # raftstore.store-pool-size: 2
    # 单个日志最大大小，硬限制。默认值：8MB；单位：MB|GB
    raftstore.raft-entry-max-size: 1GB
    # readpool.unified 该线程池自统一处理读请求的线程池， 自4.0 版本起取代原有的 storage 和 coprocessor 线程池。
    readpool.storage.use-unified-pool: true
    readpool.coprocessor.use-unified-pool: true
    # readpool.unified.max-thread-count 统一处理读请求的线程池最多的线程数量，负责处理所有的读取请求。默认配置大小为机器 CPU 数的 80% （如机器为 16 核，则默认线程池大小为 12）
    # 通常建议根据业务负载特性调整其 CPU 使用率在线程池大小的 60%～90% 之间 (如果用户 Grafana 上 TiKV-Details.Thread CPU.Unified read pool CPU 的峰值不超过 800%， 那么建议用户将 readpool.unified.max-thread-count 设置为 10，过多的线程数会造成更频繁的线程切换，并且抢占其他线程池的资源)。
  tiflash:
    # 如果节点上有多块规格不同的硬盘，推荐把 I/O 性能较好的硬盘目录配置在 storage.latest.dir 中，把 I/O 性能较一般的硬盘目录配置在 storage.main.dir
    # storage.latest.dir 用于存储最新的数据，大约占总数据量的 10% 以内；性能好的磁盘，能起到缓冲的作用
    storage.latest.dir:
      - "/nvme_ssd_a/data/tiflash"
    # storage.main.dir 用于存储主要的数据，该目录列表中的数据占总数据的 90% 以上
    storage.main.dir:
      - "/sata_ssd_b/data/tiflash"
      - "/sata_ssd_c/data/tiflash"
  pd:
    # 关闭遥测
    dashboard.enable-telemetry: false
    # 按机架和主机进行 region 调度
    # replication.max-replicas 所有副本数量，即 leader 与 follower 数量之和。默认为 3，即 1 个 leader 和 2 个 follower
    # TiKV 的 label 调度列表
    replication.location-labels: [ "rack", "host" ]
    # TiKV 集群的最小强制拓扑隔离级别
    # 为什么设置为 rack，出于资源和可用性的平衡，当某个 rack 整体出现故障，location-isolation-level 为空时，schedule.max-store-down-time 时间内无法恢复，PD 会开始调度“补充副本数”，此时可能会有大量数据迁移到其它节点，空间不够，造成服务不可用。
    replication.location-isolation-level: "rack"
    # 打开强制 TiKV Label 和 PD 的 location-labels 是否匹配的检查；默认值：false
    replication.lstrictly-match-label: true
    # schedule.max-store-down-time PD 认为失联 store（TiKV） 无法恢复的时间，当超过指定的时间没有收到 store 的心跳后，PD 会在其他节点补充副本；默认值：30m（30分钟）
    # 最佳实践：如果能确定这个节点的故障是不可恢复的，可以立即做下线处理，这样 PD 能尽快补齐副本，降低数据丢失的风险。与之相对，如果确定这个节点是能恢复的，但可能半小时之内来不及，则可以把 max-store-down-time 临时调整为比较大的值，这样能避免超时之后产生不必要的副本补充，造成资源浪费。

# 以下 host 中tc-* 改成实际 IP

# PD 实例的配置：https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference#pd_servers
pd_servers:
  - host: tc-pd-01
  - host: tc-pd-02
  - host: tc-pd-03

# TiDB 实例的配置：https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference#tidb_servers
tidb_servers:
  - host: tc-tidb-01
  - host: tc-tidb-02

# TiKV 实例的配置：https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference#tikv_servers
tikv_servers:
  - host: tc-tikv-01
    config:
      server.labels:
        rack: "r1"
        host: "tc-tikv-01"
  - host: tc-tikv-02
    config:
      server.labels:
        rack: "r1"
        host: "tc-tikv-02"
  - host: tc-tikv-03
    config:
      server.labels:
        rack: "r2"
        host: "tc-tikv-03"
  - host: tc-tikv-04
    config:
      server.labels:
        rack: "r2"
        host: "tc-tikv-04"
  - host: tc-tikv-05
    config:
      server.labels:
        rack: "r3"
        host: "tc-tikv-05"
  - host: tc-tikv-06
    config:
      server.labels:
        rack: "r3"
        host: "tc-tikv-06"

# TiFlash 实例的配置：https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference#tiflash_servers
tiflash_servers:
  - host: tc-tiflash-01
  - host: tc-tiflash-02
  - host: tc-tiflash-03
  - host: tc-tiflash-04

# CDC 实例的配置：https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference#cdc_servers
cdc_servers:
  - host: tc-ticdc-01
  - host: tc-ticdc-02

# Prometheus 实例的配置：https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference#monitoring_servers
monitoring_servers:
  - host: tc-monitor

# Grafana 实例的配置：https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference#grafana_servers
grafana_servers:
  - host: tc-monitor

# Alertemanager 实例的配置：https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference#alertmanager_servers
alertmanager_servers:
  - host: tc-monitor